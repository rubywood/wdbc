{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pylab \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('wdbc.csv', names=['ID','Diagnosis','MeanRadius','MeanTexture','MeanPerimeter','MeanArea',\n",
    "                                    'MeanSmoothness','MeanCompactness','MeanConcavity','MeanConcavePoints',\n",
    "                                    'MeanSymmetry','MeanFractalDimension','RadiusSE','TextureSE','PerimeterSE',\n",
    "                                    'AreaSE','SmoothnessSE','CompactnessSE','ConcavitySE','ConcavePointsSE',\n",
    "                                    'SymmetrySE','FractalDimensionSE','WorstRadius','WorstTexture','WorstPerimeter',\n",
    "                                    'WorstArea','WorstSmoothness','WorstCompactness','WorstConcavity',\n",
    "                                    'WorstConcave Points','WorstSymmetry','WorstFractalDimension'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(df)\n",
    "df.info() #no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Diagnosis'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the mean, median and standard deviation of the “perimeter” feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mean of the MeanPerimeter column is 91.97, from df.describe() table above.\n",
    "# Median of MeanPerimeter is 86.24, and standard deviation of MeanPerimeter is 24.30.\n",
    "df.MeanPerimeter.plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the first feature in this data set (the “radius”) normally distributed? Please quantitatively define you answer. If not, what might be a more appropriate distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.MeanRadius.plot(kind='box');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.MeanRadius.plot(kind='hist', bins=20);\n",
    "# Based on a histogram of the mean radius values, this feature does not look normally distributed.\n",
    "# This distribution is right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.probplot(df.MeanRadius, dist=\"norm\", plot=pylab)\n",
    "pylab.show()\n",
    "# The QQ plot suggests the radius feature is not normal, as the blue data starts above the normal line, goes below, and \n",
    "# then goes back above, indicating some underlying patterns that are not well represented by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shapiro-Wilk Test - a hypothesis test for if the distibution is normal\n",
    "print('Null hypothesis: the radius is normally distributed.')\n",
    "seed(1234)\n",
    "stat, p = shapiro(df.MeanRadius)\n",
    "print('Test Statistic = %.3f, p-value = %.6f.' % (stat, p))\n",
    "# The p-value is neglibile for this test of the null hypothesis, and therefore at any level of signifcance we can reject the\n",
    "# null hypothesis, and conclude that the radius is not normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more appropriate distribution might be a Gamma distribution, or possibly an F distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.001, 35, 5000)\n",
    "f, ax_arr = plt.subplots(2, 2, figsize=(14,7))\n",
    "\n",
    "# Histogram\n",
    "ax_arr[0,0].hist(df.MeanRadius, bins=20)\n",
    "ax_arr[0,0].set_title('Histogram of MeanRadius')\n",
    "\n",
    "# F distribution\n",
    "yF=stats.f.pdf(x, 10, 100, 0, 11)\n",
    "ax_arr[0,1].plot(x, yF);\n",
    "ax_arr[0,1].set_title('F distribution')\n",
    "# I played around with the parameters of the F distribution pdf, and found this combination gives a distribution which looks\n",
    "# similar to our histogram of the MeanRadius.\n",
    "\n",
    "# Gamma distribution\n",
    "yGamma=stats.gamma.pdf(x, 14)\n",
    "ax_arr[1,0].plot(x, yGamma);\n",
    "ax_arr[1,0].set_title('Gamma distribution')\n",
    "# The mean of MeanRadius is 14.13. When I use 14 as the shape parameter here, this gamma distribution looks similar to our \n",
    "# histogram of MeanRadius.\n",
    "\n",
    "ax_arr[1,1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One could check if these Gamma/F distributions are appropriate using a Kolmogorov–Smirnov test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier to predict the diagnosis of malignant or benign. Please compare the results of two classifiers e.g. SVM, logistic regression, decision tree etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deal with missing values (none), outliers and feature engineering.\n",
    "# Look at ranges of all Mean features - do I need to standardize?\n",
    "dfMeans = df[['Diagnosis','MeanRadius','MeanTexture','MeanPerimeter','MeanArea',\n",
    "                                    'MeanSmoothness','MeanCompactness','MeanConcavity','MeanConcavePoints',\n",
    "                                    'MeanSymmetry','MeanFractalDimension']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfMeans.describe()\n",
    "# Many values always <1 but others larger, in particular MeanArea has a mean of 654.89 with largest value 2501."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Expect correlation/collinearity between Radius, Area, Perimeter and Compactness (perimeter^2 / area - 1.0)/\n",
    "# As area takes such large values, could log or sqrt transform, but as area = pi * radius**2, could just use radius instead.\n",
    "f, ax_arr = plt.subplots(3, 2, figsize=(14,7))\n",
    "\n",
    "ax_arr[0,0].scatter(dfMeans.MeanRadius, dfMeans.MeanArea);\n",
    "ax_arr[0,0].set_xlabel('MeanRadius')\n",
    "ax_arr[0,0].set_ylabel('MeanArea')\n",
    "\n",
    "ax_arr[0,1].scatter(dfMeans.MeanRadius, dfMeans.MeanPerimeter); \n",
    "ax_arr[0,1].set_xlabel('MeanRadius')\n",
    "ax_arr[0,1].set_ylabel('MeanPerimeter')\n",
    "\n",
    "ax_arr[1,0].scatter(dfMeans.MeanRadius, dfMeans.MeanCompactness);\n",
    "ax_arr[1,0].set_xlabel('MeanRadius')\n",
    "ax_arr[1,0].set_ylabel('MeanCompactness')\n",
    "\n",
    "ax_arr[1,1].scatter(dfMeans.MeanArea, dfMeans.MeanPerimeter);\n",
    "ax_arr[1,1].set_xlabel('MeanArea')\n",
    "ax_arr[1,1].set_ylabel('MeanPerimeter')\n",
    "\n",
    "ax_arr[2,0].scatter(dfMeans.MeanArea, dfMeans.MeanCompactness);\n",
    "ax_arr[2,0].set_xlabel('MeanArea')\n",
    "ax_arr[2,0].set_ylabel('MeanCompactness')\n",
    "\n",
    "ax_arr[2,1].scatter(dfMeans.MeanPerimeter, dfMeans.MeanCompactness);\n",
    "ax_arr[2,1].set_xlabel('MeanPerimeter')\n",
    "ax_arr[2,1].set_ylabel('MeanCompactness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Suggests collinearity between radius and area, radius and perimeter, and perimeter and area.\n",
    "# In context, I think radius is best feature to use out of all of these. Then will use compactness as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def outlierPlots(column, columnName):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,4))\n",
    "    ax1.hist(column, bins=20);\n",
    "    ax1.set_title(columnName)\n",
    "    ax2.boxplot(column);\n",
    "    ax2.set_title(columnName)\n",
    "    plt.show()\n",
    "\n",
    "outlierPlots(dfMeans.MeanRadius, 'MeanRadius')\n",
    "outlierPlots(dfMeans.MeanTexture, 'MeanTexture')\n",
    "outlierPlots(dfMeans.MeanSmoothness, 'MeanSmoothness')\n",
    "outlierPlots(dfMeans.MeanCompactness, 'MeanCompactness')\n",
    "outlierPlots(dfMeans.MeanConcavity, 'MeanConcavity')\n",
    "outlierPlots(dfMeans.MeanConcavePoints, 'MeanConcavePoints')\n",
    "outlierPlots(dfMeans.MeanSymmetry, 'MeanSymmetry')\n",
    "outlierPlots(dfMeans.MeanFractalDimension, 'MeanFractalDimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outlierRange = (dfMeans.quantile(0.75) - dfMeans.quantile(0.25)) * 1.5\n",
    "outlierHighBoundary = dfMeans.quantile(0.75) + outlierRange\n",
    "outlierLowBoundary = dfMeans.quantile(0.25) - outlierRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outliers = dfMeans[(dfMeans.MeanFractalDimension > outlierHighBoundary.MeanFractalDimension) & (dfMeans.MeanFractalDimension < outlierLowBoundary.MeanFractalDimension)]\n",
    "# no outliers (under this definition) for any of the 'Mean...' variables\n",
    "len(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into test and training datasets: 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into test and training datasets: 80/20\n",
    "X = dfMeans.loc[:,'MeanRadius':].as_matrix()\n",
    "y = dfMeans['Diagnosis'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Diagnosis counts in train: \"M\" has {0:.0f}, \"B\" has {1:.0f}. Ratio of \"M\" to \"B\" is {2:.3f}.'\n",
    "      .format(np.count_nonzero(y_train == 'M'),np.count_nonzero(y_train == 'B'),\n",
    "             np.count_nonzero(y_train == 'M')/np.count_nonzero(y_train == 'B')))\n",
    "print('Diagnosis counts in train: \"M\" has {0:.0f}, \"B\" has {1:.0f}. Ratio of \"M\" to \"B\" is {2:.3f}.'\n",
    "      .format(np.count_nonzero(y_test == 'M'),np.count_nonzero(y_test == 'B'),\n",
    "             np.count_nonzero(y_test == 'M')/np.count_nonzero(y_test == 'B')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model with all Mean variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Score for logisitic regression model is {0:.2f}'.format(model_lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Performance metrics\n",
    "print('Accuracy for logisitic regression model is {0:.2f}'.format(accuracy_score(y_test, model_lr.predict(X_test))))\n",
    "print('Confusion matrix for logisitic regression model is \\n {0}'.format(confusion_matrix(y_test, model_lr.predict(X_test))))\n",
    "# Need to change 'M', 'B' to numeric values for precision and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model coefficients for each feature\n",
    "model_lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model with fewer Mean variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing collinear varibles - keeping radius only\n",
    "dfMeansFewer = dfMeans[dfMeans.columns.difference(['MeanArea', 'MeanPerimeter'])]\n",
    "dfMeansFewer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2 = dfMeansFewer.loc[:,'MeanCompactness':].as_matrix()\n",
    "y2 = dfMeansFewer['Diagnosis'].ravel()\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr2 = LogisticRegression(random_state=1)\n",
    "model_lr2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Score for logisitic regression model is {0:.2f}'.format(model_lr2.score(X_test2, y_test2)))\n",
    "print('Accuracy for logisitic regression model is {0:.2f}'.format(accuracy_score(y_test2, model_lr2.predict(X_test2))))\n",
    "print('Confusion matrix for logisitic regression model is \\n {0}'.format(confusion_matrix(y_test2, model_lr2.predict(X_test2))))\n",
    "# Using fewer variables gives same accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation of Logistic Regression Model with fewer Mean variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr3 = LogisticRegression(random_state=1)\n",
    "parameters = {'C':[1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty': ['l1', 'l2']}\n",
    "# 3 fold cross validation\n",
    "clf = GridSearchCV(model_lr3, param_grid=parameters, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Score for logistic regression with hyperparameter optimization is {0:.02f}'.format(clf.score(X_test2, y_test2)))\n",
    "print('Accuracy for logistic regression with hyperparameter optimization is {0:.02f}'\n",
    "      .format(accuracy_score(y_test2, clf.predict(X_test2))))\n",
    "print('Confusion matrix for logistic regression with hyperparameter optimization is \\n {0}'\n",
    "      .format(confusion_matrix(y_test2, clf.predict(X_test2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation of Logistic Regression Model with fewer Mean variables (standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train2)\n",
    "X_test_scaled = scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_lr4 = LogisticRegression(random_state=1)\n",
    "clf2 = GridSearchCV(model_lr4, param_grid=parameters, cv=3)\n",
    "clf2.fit(X_train_scaled, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Score for logistic regression with hyperparameter optimization and standardized variables is {0:.02f}'\n",
    "      .format(clf2.score(X_test_scaled, y_test2)))\n",
    "print('Accuracy for logistic regression with hyperparameter optimization and standardized variables is {0:.02f}'\n",
    "      .format(accuracy_score(y_test2, clf2.predict(X_test_scaled))))\n",
    "print('Confusion matrix for logistic regression with hyperparameter optimization and standardized variables is \\n {0}'\n",
    "      .format(confusion_matrix(y_test2, clf2.predict(X_test_scaled))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So standardizing variables gives a slightly worse 'best' score, but better 'score' and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=10, random_state=1, max_features=None, min_samples_leaf=15)\n",
    "dfit = dtree.fit(X_train2, y_train2)\n",
    "dfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = dtree.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Score for decision tree model is {0:.2f}'.format(model_lr2.score(X_test2, y_test2)))\n",
    "print('Accuracy for decision tree model is {0:.2f}'.format(accuracy_score(y_test2, y_pred)))\n",
    "print('Confusion matrix for decision tree model is \\n {0}'.format(confusion_matrix(y_test2, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(dfit, out_file='tree.dot', rounded=True, proportion=False, filled=True)\n",
    "!dot -Tpng tree.dot -o tree.png -Gdpi=600\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation of Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "treeParameters = {'max_depth':[8, 10, 12], 'min_samples_leaf': [10, 15, 20]}\n",
    "clf3 = GridSearchCV(dtree, param_grid=treeParameters, cv=3)\n",
    "clf3.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Score for cross validation of decision tree is {0:.02f}'.format(clf3.score(X_test2, y_test2)))\n",
    "print('Accuracy for decision tree model is {0:.2f}'.format(accuracy_score(y_test2, dtree.predict(X_test2))))\n",
    "print('Confusion matrix for decision tree model is \\n {0}'.format(confusion_matrix(y_test2, dtree.predict(X_test2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "#### In this example, the model that performed best was found when doing Cross Validation of Logistic Regression Model with fewer Mean variables (standardized).\n",
    "#### Also, it is not a surprise that using cross validation to compare different model parameters improved performance metrics in all scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
